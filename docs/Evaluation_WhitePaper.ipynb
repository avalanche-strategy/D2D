{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "70bf40e1",
   "metadata": {},
   "source": [
    "# RAG Answer Evaluation Framework — White Paper\n",
    "\n",
    "This document explains the evaluation framework for the answers generated by D2D models. It covers the purpose and scoring logic for each metric, formula explanation for complex metrics, real cases from evaluation by golden samples, and a summary table for quick reference."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f53d740",
   "metadata": {},
   "source": [
    "## Evaluation Metrics Overview\n",
    "\n",
    "We use **five core metrics** to assess the quality of answers generated by a LLM with retrieval (RAG):\n",
    "\n",
    "1. **Correctness** – Is the answer consistent with the reference (ground truth)?\n",
    "2. **Faithfulness** – Is the answer fully supported by the retrieved context (avoiding hallucinations)?\n",
    "3. **Precision** – How much of the answer is actually supported by retrieved chunks?\n",
    "4. **Recall** – How many facts are covered by the retrieved chunks?\n",
    "5. **Relevance** – Is the answer relevant to the original guideline question?\n",
    "\n",
    "\n",
    "\n",
    "## 1. Correctness\n",
    "\n",
    "### Goal:\n",
    "Evaluate how factually **aligned** the generated answer is with the **reference answer**.\n",
    "\n",
    "### Logic:\n",
    "- Score is given on a **1–5 scale**.\n",
    "- LLM compares the **RAG-generated answer** with the **reference answer** and evaluates factual alignment.\n",
    "\n",
    "### Use Case:\n",
    "Evaluates **truthfulness** against known answers."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7bf57aaa",
   "metadata": {},
   "source": [
    "## 2. Faithfulness\n",
    "\n",
    "### Goal:\n",
    "Determine whether the answer **hallucinates** or is entirely supported by the retrieved context.\n",
    "\n",
    "### Logic:\n",
    "\n",
    "``` ini\n",
    "faithfulness = # of supported facts in answer / total # of facts in answer\n",
    "```\n",
    "\n",
    "- If context is missing → faithfulness = 1.0 with warning: “No context to verify.”\n",
    "- If both reference and answer are “confused” → judge tone/attitude.\n",
    "- Score is converted from [0–1] to [1–5] via:  scaled_score = 1 + 4 × raw_score\n",
    "\n",
    "### Flowchart:\n",
    "```mermaid\n",
    "flowchart TD\n",
    "    A[Generated Answer] --> B[Extract all factual claims]\n",
    "    B --> C[Check each claim against retrieved context]\n",
    "    C --> D[Count supported and hallucinated]\n",
    "    D --> E[Compute ratio: supported / total]\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a40bc82e",
   "metadata": {},
   "source": [
    "## 3. Precision\n",
    "\n",
    "### Goal:\n",
    "Assess how much of the answer is actually grounded in retrieved context chunks.\n",
    "\n",
    "### Logic:\n",
    "\n",
    "``` ini\n",
    "precision = # related chunks / total retrieval chunks\n",
    "```\n",
    "\n",
    "- Precision is chunk-level: whether what the LLM says appears in any chunk.\n",
    "- If the answer is indistinct, compare chunks to reference answer.\n",
    "- If no retrieval context → precision = NaN."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ad97fda",
   "metadata": {},
   "source": [
    "## 4. Recall\n",
    "\n",
    "### Goal:\n",
    "Check whether the retrieved content contains the information needed to reconstruct the reference answer.\n",
    "\n",
    "### Logic:\n",
    "\n",
    "``` ini\n",
    "recall = # covered facts (i.e.  total facts from retriever ∩ total factors in the generated answer)  / total facts in the generated answer\n",
    "```\n",
    "\n",
    "- Recall evaluates retrieval quality: was enough information retrieved?\n",
    "- If the answer is indistinct, use reference answer instead.\n",
    "- If no context → recall = 1.0."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08cfce45",
   "metadata": {},
   "source": [
    "## 5. Relevance\n",
    "\n",
    "### Goal:\n",
    "Determine if the answer is on-topic with the original guideline question.\n",
    "\n",
    "### Logic:\n",
    "- Score is given on a **1–5 scale**.\n",
    "- LLM checks whether the answer addresses the intent of the question.\n",
    "\n",
    "---\n",
    "\n",
    "## Summary Table\n",
    "| Metric      | Goal           | Scale      |  Analogy               |  Weithting  Suggesting |\n",
    "|-------------|----------------|------------|------------------------|------------------------|\n",
    "| Correctness | Answer alignment | 1–5       | How many points the student earns by taking an exam with a cheat sheet.  | 0.4 (high)         |\n",
    "| Faithfulness| Hallucination detection | 0–1 -> 1-5     | Whether the student copied only from the cheat sheet without making things up. | 0.2 (medium)       |\n",
    "| Precision   | Context relevance | 0–1 -> 1-5     | How much of the cheat sheet content was actually used in the answer. | 0.2 (medium)       |\n",
    "| Recall      | Information coverage | 0–1 -> 1-5     | Whether the student brought a complete cheat sheet to the test. | 0.2 (medium)       |\n",
    "| Relevance   | Question alignment | 1–5       | Whether the student answered the question that was actually asked. (For questionaire quality purpose, not model performance)| 0 (low)         |\n",
    "\n",
    "Note: If both the answer and reference are “confused”, the correctness score should be high because the answers are highly similar. However the relevance score should be low, indicating the answer is not relevant to the question.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82ce5c7d",
   "metadata": {},
   "source": [
    "## 6. Case Study: Metric Demonstration\n",
    "\n",
    "Let’s consider the following example from the dataset:\n",
    "\n",
    "**Question:**\n",
    "> Do you think this kind of technology is exciting or concerning?\n",
    "\n",
    "**Generated Answer:**\n",
    "> Game changer for disabilities, but emotionally exhausting.\n",
    "\n",
    "**Reference Answer (Ground Truth):**\n",
    "> Mostly exciting, though there are some risks.\n",
    "\n",
    "**Retrieved Context:**\n",
    "> chunk 1:\n",
    "Interviewer: What’s a benefit you can imagine?\n",
    "Interviewee: For people with disabilities—especially speech disabilities—it could be a game changer. Also might help people with anxiety express themselves more easily.\n",
    "\n",
    "> chunk 2:\n",
    "Interviewer: And a possible downside?\n",
    "Interviewee: Mental fatigue maybe? Or like, accidentally leaking a private thought. I think it could be emotionally exhausting to always filter your own brain.\n",
    "\n",
    "> chunk 3:\n",
    "Interviewer: Could this tech exist soon?\n",
    "Interviewee: I mean, we're not *that* far off. With the pace of brain-computer interface research? It’s coming. So yeah, maybe within our lifetime.\n",
    "\n",
    "### Metric Breakdown:\n",
    "\n",
    "| Metric        | Score | Explanation |\n",
    "|---------------|-------|-------------|\n",
    "| **Correctness** | 3.0 | The answer overlaps partially with the reference: both acknowledge benefits and risks, but not fully aligned. |\n",
    "| **Faithfulness** | 5.0 | Total facts: 2 (\"Game changer for disabilities\" and \"emotionally exhausting\")<br>Supported facts: 2<br>Hallucinated facts: 0<br>Faithfulness = 2 / 2 = 1 → projected to 5.0 |\n",
    "| **Precision** | 3.7 | Total chunks: 3<br>Used chunks: chunk 1, chunk 2<br>Unused chunks: chunk 3<br>Precision = 2 / 3 = 0.67 → projected to 3.7 |\n",
    "| **Recall** | 5.0 | Total facts: 2 (\"game changer\" for disabilities (chunk 1), and \"emotionally exhausting\" (chunk 2))<br>Covered facts: 2<br>Uncovered facts: None<br>Recall = 2 / 2 = 1 → projected to 5.0 |\n",
    "| **Relevance** | 4.0 | The answer is mostly on-topic and addresses the question, though slightly indirect. |\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:d2d]",
   "language": "python",
   "name": "conda-env-d2d-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
