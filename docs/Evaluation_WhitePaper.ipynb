{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "70bf40e1",
   "metadata": {},
   "source": [
    "# RAG Answer Evaluation Framework — White Paper\n",
    "\n",
    "This document explains the evaluation framework for the answers generated by D2D models. It covers the purpose and scoring logic for each metric, formula explanation for complex metrics, real cases from evaluation by golden samples, and a summary table for quick reference.\n",
    "\n",
    "---\n",
    "\n",
    "## Evaluation Metrics Overview\n",
    "\n",
    "We use **five core metrics** to assess the quality of answers generated by a LLM with retrieval (RAG):\n",
    "\n",
    "1. **Correctness** – Is the answer consistent with the reference (ground truth)?\n",
    "2. **Faithfulness** – Is the answer fully supported by the retrieved context (avoiding hallucinations)?\n",
    "3. **Precision** – How much of the answer is actually supported by retrieved chunks?\n",
    "4. **Recall** – How many facts are covered by the retrieved chunks?\n",
    "5. **Relevance** – Is the answer relevant to the original guideline question?\n",
    "\n",
    "---\n",
    "\n",
    "## 1. Correctness\n",
    "\n",
    "### Goal:\n",
    "Evaluate how factually **aligned** the generated answer is with the **reference answer**.\n",
    "\n",
    "### Logic:\n",
    "- Score is given on a **1–5 scale**.\n",
    "- LLM compares the **RAG-generated answer** with the **reference answer** and evaluates factual alignment.\n",
    "\n",
    "### Use Case:\n",
    "Evaluates **truthfulness** against known answers.\n",
    "\n",
    "### Analogy:\n",
    "Imagine the LLM is a student copying the homework (from retrieval information), and the **reference answer** is the standard answer key.  \n",
    "**Correctness** measures how many points the student would get — did they copy the key ideas, provide the right facts, and reach the same conclusion.\n",
    "\n",
    "---\n",
    "\n",
    "## 2. Faithfulness\n",
    "\n",
    "### Goal:\n",
    "Determine whether the answer **hallucinates** or is entirely supported by the retrieved context.\n",
    "\n",
    "### Logic:\n",
    "\n",
    "``` ini\n",
    "faithfulness = # of supported facts in answer / total # of facts in answer\n",
    "```\n",
    "\n",
    "- If context is missing → faithfulness = 1.0 with warning: “No context to verify.”\n",
    "- If both reference and answer are “confused” → judge tone/attitude.\n",
    "- Score is converted from [0–1] to [1–5] via:  scaled_score = 1 + 4 × raw_score\n",
    "\n",
    "### Flowchart:\n",
    "```mermaid\n",
    "flowchart TD\n",
    "    A[Generated Answer] --> B[Extract all factual claims]\n",
    "    B --> C[Check each claim against retrieved context]\n",
    "    C --> D[Count supported and hallucinated]\n",
    "    D --> E[Compute ratio: supported / total]\n",
    "```\n",
    "\n",
    "### Analogy:\n",
    "Faithfulness is like how exactly a student copies from the material. If they add made-up content, that’s hallucination.\n",
    "\n",
    "---\n",
    "\n",
    "## 3. Precision\n",
    "\n",
    "### Goal:\n",
    "Assess how much of the answer is actually grounded in retrieved context chunks.\n",
    "\n",
    "### Logic:\n",
    "\n",
    "``` ini\n",
    "precision = # related chunks / total retrieval chunks\n",
    "```\n",
    "\n",
    "- Precision is chunk-level: whether what the LLM says appears in any chunk.\n",
    "- If no context → precision = NaN.\n",
    "\n",
    "---\n",
    "\n",
    "## 4. Recall\n",
    "\n",
    "### Goal:\n",
    "Check whether the retrieved content contains the information needed to reconstruct the reference answer.\n",
    "\n",
    "### Logic:\n",
    "\n",
    "``` ini\n",
    "recall = # covered facts (i.e.  total facts from retriever ∩ total factors in the generated answer)  / total facts in the generated answer\n",
    "```\n",
    "\n",
    "- Recall evaluates retrieval quality: was enough information retrieved?\n",
    "- If no context → recall = 1.0.\n",
    "\n",
    "### Analogy:\n",
    "Recall is about whether you found the full cheat sheet. By contrast, faithfulness is whether you copied from it faithfully.\n",
    "\n",
    "---\n",
    "\n",
    "## 5. Relevance\n",
    "\n",
    "### Goal:\n",
    "Determine if the answer is on-topic with the original guideline question.\n",
    "\n",
    "### Logic:\n",
    "- Score is given on a **1–5 scale**.\n",
    "- LLM checks whether the answer addresses the intent of the question.\n",
    "\n",
    "---\n",
    "\n",
    "## Summary Table\n",
    "| Metric      | Goal           | Scale      |  Analogy               |  Weithting  Suggesting |\n",
    "|-------------|----------------|------------|------------------------|------------------------|\n",
    "| Correctness | Answer alignment | 1–5       | How many points the student earns by copying from their notes, compared to the answer key.  | 0.4 (high)         |\n",
    "| Faithfulness| Hallucination detection | 1–5       | How accurately the student copied from the material, without adding made-up content. | 0.2 (medium)       |\n",
    "| Precision   | Context relevance | 0–1 -> 1-5     | The proportion of the cheat sheet the student used to support their answer. | 0.15 (medium)       |\n",
    "| Recall      | Information coverage | 0–1 -> 1-5     | Whether the student found the full cheat sheet to answer the question. | 0.15 (medium)       |\n",
    "| Relevance   | Question alignment | 1–5       | How well the student’s answer matches the question intent. | 0.1 (low)         |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0c3a97e",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
