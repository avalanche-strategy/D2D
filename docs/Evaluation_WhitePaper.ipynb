{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "70bf40e1",
   "metadata": {},
   "source": [
    "# RAG Answer Evaluation Framework — White Paper\n",
    "\n",
    "This document explains the evaluation framework for the answers generated by D2D models. It covers the purpose and scoring logic for each metric, formula explanation for complex metrics, real cases from evaluation by golden samples, and a summary table for quick reference.\n",
    "\n",
    "---\n",
    "\n",
    "## Evaluation Metrics Overview\n",
    "\n",
    "We use **five core metrics** to assess the quality of answers generated by a LLM with retrieval (RAG):\n",
    "\n",
    "1. **Correctness** – Is the answer consistent with the reference (ground truth)?\n",
    "2. **Faithfulness** – Is the answer fully supported by the retrieved context (avoiding hallucinations)?\n",
    "3. **Precision** – How much of the answer is actually supported by retrieved chunks?\n",
    "4. **Recall** – How many facts are covered by the retrieved chunks?\n",
    "5. **Relevance** – Is the answer relevant to the original guideline question?\n",
    "\n",
    "---\n",
    "\n",
    "## 1. Correctness\n",
    "\n",
    "### Goal:\n",
    "Evaluate how factually **aligned** the generated answer is with the **reference answer**.\n",
    "\n",
    "### Logic:\n",
    "- Score is given on a **1–5 scale**.\n",
    "- LLM compares the **RAG-generated answer** with the **reference answer** and evaluates factual alignment.\n",
    "\n",
    "### Use Case:\n",
    "Evaluates **truthfulness** against known answers.\n",
    "\n",
    "### Analogy:\n",
    "Imagine the LLM is a student copying the homework (from retrieval information), and the **reference answer** is the standard answer key.  \n",
    "**Correctness** measures how many points the student would get — did they copy the key ideas, provide the right facts, and reach the same conclusion.\n",
    "\n",
    "---\n",
    "\n",
    "## 2. Faithfulness\n",
    "\n",
    "### Goal:\n",
    "Determine whether the answer **hallucinates** or is entirely supported by the retrieved context.\n",
    "\n",
    "### Logic:\n",
    "\n",
    "``` ini\n",
    "faithfulness = # of supported facts in answer / total # of facts in answer\n",
    "```\n",
    "\n",
    "- If context is missing → faithfulness = 1.0 with warning: “No context to verify.”\n",
    "- If both reference and answer are “confused” → judge tone/attitude.\n",
    "- Score is converted from [0–1] to [1–5] via:  scaled_score = 1 + 4 × raw_score\n",
    "\n",
    "### Flowchart:\n",
    "```mermaid\n",
    "flowchart TD\n",
    "    A[Generated Answer] --> B[Extract all factual claims]\n",
    "    B --> C[Check each claim against retrieved context]\n",
    "    C --> D[Count supported and hallucinated]\n",
    "    D --> E[Compute ratio: supported / total]\n",
    "```\n",
    "\n",
    "### Analogy:\n",
    "Faithfulness is like how exactly a student copies from the material. If they add made-up content, that’s hallucination.\n",
    "\n",
    "---\n",
    "\n",
    "## 3. Precision\n",
    "\n",
    "### Goal:\n",
    "Assess how much of the answer is actually grounded in retrieved context chunks.\n",
    "\n",
    "### Logic:\n",
    "\n",
    "``` ini\n",
    "precision = # related chunks / total retrieval chunks\n",
    "```\n",
    "\n",
    "- Precision is chunk-level: whether what the LLM says appears in any chunk.\n",
    "- If the answer is indistinct, compare chunks to reference answer.\n",
    "- If no retrieval context → precision = NaN.\n",
    "\n",
    "---\n",
    "\n",
    "## 4. Recall\n",
    "\n",
    "### Goal:\n",
    "Check whether the retrieved content contains the information needed to reconstruct the reference answer.\n",
    "\n",
    "### Logic:\n",
    "\n",
    "``` ini\n",
    "recall = # covered facts (i.e.  total facts from retriever ∩ total factors in the generated answer)  / total facts in the generated answer\n",
    "```\n",
    "\n",
    "- Recall evaluates retrieval quality: was enough information retrieved?\n",
    "- If the answer is indistinct, use reference answer instead.\n",
    "- If no context → recall = 1.0.\n",
    "\n",
    "### Analogy:\n",
    "Recall is about whether you found the full cheat sheet. By contrast, faithfulness is whether you copied from it faithfully.\n",
    "\n",
    "---\n",
    "\n",
    "## 5. Relevance\n",
    "\n",
    "### Goal:\n",
    "Determine if the answer is on-topic with the original guideline question.\n",
    "\n",
    "### Logic:\n",
    "- Score is given on a **1–5 scale**.\n",
    "- LLM checks whether the answer addresses the intent of the question.\n",
    "\n",
    "---\n",
    "\n",
    "## Summary Table\n",
    "| Metric      | Goal           | Scale      |  Analogy               |  Weithting  Suggesting |\n",
    "|-------------|----------------|------------|------------------------|------------------------|\n",
    "| Correctness | Answer alignment | 1–5       | How many points the student earns by copying from the notes, compared to the answer key.  | 0.4 (high)         |\n",
    "| Faithfulness| Hallucination detection | 1–5       | How accurately the student copied from the material, without adding made-up content. | 0.2 (medium)       |\n",
    "| Precision   | Context relevance | 0–1 -> 1-5     | The proportion of the cheat sheet the student used to support their answer. | 0.15 (medium)       |\n",
    "| Recall      | Information coverage | 0–1 -> 1-5     | Whether the student found the full cheat sheet to answer the question. | 0.15 (medium)       |\n",
    "| Relevance   | Question alignment | 1–5       | How well the student’s answer matches the question intent. | 0.1 (low)         |\n",
    "\n",
    "---\n",
    "\n",
    "## 6. Case Study: Metric Demonstration\n",
    "\n",
    "Let’s consider the following example from the dataset:\n",
    "\n",
    "**Question:**\n",
    "> Do you think this kind of technology is exciting or concerning?\n",
    "\n",
    "**Generated Answer:**\n",
    "> Game changer for disabilities, but emotionally exhausting.\n",
    "\n",
    "**Reference Answer (Ground Truth):**\n",
    "> Mostly exciting, though there are some risks.\n",
    "\n",
    "**Retrieved Context:**\n",
    "> chunk 1:\n",
    "Interviewer: What’s a benefit you can imagine?\n",
    "Interviewee: For people with disabilities—especially speech disabilities—it could be a game changer. Also might help people with anxiety express themselves more easily.\n",
    "\n",
    "> chunk 2:\n",
    "Interviewer: And a possible downside?\n",
    "Interviewee: Mental fatigue maybe? Or like, accidentally leaking a private thought. I think it could be emotionally exhausting to always filter your own brain.\n",
    "\n",
    "> chunk 3:\n",
    "Interviewer: Could this tech exist soon?\n",
    "Interviewee: I mean, we're not *that* far off. With the pace of brain-computer interface research? It’s coming. So yeah, maybe within our lifetime.\n",
    "\n",
    "### Metric Breakdown:\n",
    "\n",
    "| Metric        | Score | Explanation |\n",
    "|---------------|-------|-------------|\n",
    "| **Correctness** | 3.0 | The answer overlaps partially with the reference: both acknowledge benefits and risks, but not fully aligned. |\n",
    "| **Faithfulness** | 5.0 | Total facts: 2 (\"Game changer for disabilities\" and \"emotionally exhausting\")<br>Supported facts: 2<br>Hallucinated facts: 0<br>Faithfulness = 2 / 2 = 1 → projected to 5.0 |\n",
    "| **Precision** | 3.7 | Total chunks: 3<br>Used chunks: chunk 1, chunk 2<br>Unused chunks: chunk 3<br>Precision = 2 / 3 = 0.67 → projected to 3.7 |\n",
    "| **Recall** | 5.0 | Total facts: 2 (\"game changer\" for disabilities (chunk 1), and \"emotionally exhausting\" (chunk 2))<br>Covered facts: 2<br>Uncovered facts: None<br>Recall = 2 / 2 = 1 → projected to 5.0 |\n",
    "| **Relevance** | 4.0 | The answer is mostly on-topic and addresses the question, though slightly indirect. |\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d851f354",
   "metadata": {},
   "source": [
    "| Metric        | Score | Explanation |\n",
    "|---------------|-------|-------------|\n",
    "| **Correctness** | 3.0 | The answer overlaps partially with the reference: both acknowledge benefits and risks, but not fully aligned. |\n",
    "| **Faithfulness** | 5.0 | Total facts: 2 (\"Game changer for disabilities\" and \"emotionally exhausting\")  \n",
    "Supported facts: 2  \n",
    "Hallucinated facts: 0  \n",
    "Faithfulness = 2 / 2 = 1 → projected to 5.0 |\n",
    "| **Precision** | 3.7 | Total chunks: 3  \n",
    "Used chunks: chunk 1, chunk 2  \n",
    "Unused chunks: chunk 3  \n",
    "Precision = 2 / 3 = 0.67 → projected to 3.7 |\n",
    "| **Recall** | 5.0 | Total facts: 2 (\"game changer\" for disabilities (chunk 1), and \"emotionally exhausting\" (chunk 2))  \n",
    "Covered facts: 2  \n",
    "Uncovered facts: None  \n",
    "Recall = 2 / 2 = 1 → projected to 5.0 |\n",
    "| **Relevance** | 4.0 | The answer is mostly on-topic and addresses the question, though slightly indirect. |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4d6bd30",
   "metadata": {},
   "source": [
    "| Metric        | Score | Explanation |\n",
    "|---------------|-------|-------------|\n",
    "| **Correctness** | 3.0 | The answer overlaps partially with the reference: both acknowledge benefits and risks, but not fully aligned. |\n",
    "| **Faithfulness** | 5.0 | Total facts: 2 (\"Game changer for disabilities\" and \"emotionally exhausting\")  \n",
    "Supported facts: 2  \n",
    "Hallucinated facts: 0  \n",
    "Faithfulness = 2 / 2 = 1 → projected to 5.0 |\n",
    "| **Precision** | 3.7 | Total chunks: 3  \n",
    "Used chunks: chunk 1, chunk 2  \n",
    "Unused chunks: chunk 3  \n",
    "Precision = 2 / 3 = 0.67 → projected to 3.7 |\n",
    "| **Recall** | 5.0 | Total facts: 2 (\"game changer\" for disabilities (chunk 1), and \"emotionally exhausting\" (chunk 2))  \n",
    "Covered facts: 2  \n",
    "Uncovered facts: None  \n",
    "Recall = 2 / 2 = 1 → projected to 5.0 |\n",
    "| **Relevance** | 4.0 | The answer is mostly on-topic and addresses the question, though slightly indirect. |\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "D2D",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
