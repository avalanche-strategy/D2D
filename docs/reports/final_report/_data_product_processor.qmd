
The processor pipeline, as shown in @fig-rag-pipeline, comprises three core components: the **Pre-processor**, the **Retriever**, and the **Generator**. These components work together to efficiently transform raw input data into meaningful, structured outputs aligned with the Dialogue2Data (D2D) projectâ€™s objectives.


### Pre-processor: LLM Summarization

The Pre-processor prepares the raw transcript for analysis by segmenting and summarizing its content.

- **Inputs**:
  - A raw interview transcript containing interviewer questions and interviewee responses.
  - A guideline providing standard questions or topics to guide the analysis.
- **Segmentation**:
  - The transcript is divided into question-and-answer (QA) pairs.
- **Summarization**:
  - The questions from each transcript QA pair and the guideline questions are summarized using a Large Language Model (LLM). This distills key points and reduces complexity, ensuring concise representations for matching.
- **Output**:
  - A list of summarized guideline questions.
  - A dictionary where each entry contains an original transcript QA pair and its summarized question.

### Retriever: Embedding and Matching

The Retriever identifies the most relevant transcript QA pairs for each guideline question.

- **Inputs**:
  - The output for the pre-processor
- **Embedding**:
  - The summarized questions from both the transcript QA pairs and the guideline are converted into semantic embeddings using `SentenceTransformer` (e.g., `multi-qa-mpnet-base-dot-v1`).
- **Similarity Calculation**:
  - Cosine similarity is computed between the embeddings of each summarized guideline question and the summarized questions from the transcript QA pairs to determine semantic relevance.
- **Selection**:
  - For each guideline question, the top-k or top-p (Top-k selects the k most similar items; top-p selects items until their cumulative similarity reaches a threshold) most similar transcript QA pairs are selected based on their summarized questions.
- **Output**:
  - A set of relevant transcript QA pairs matched to each guideline question.


### Generator: LLM Output Generation

The Generator produces concise, structured answers for each guideline question based on the matched QA pairs.

- **Inputs**:
  - The output of the retriever
- **LLM Processing**:
  - For each question in the guideline, an LLM (e.g., `gpt-4o-mini`) extracts key information from the relevant QA pairs selected from the retriever and synthesizes it into a coherent response.
- **Output Generation**:
  - Structured answers are generated in CSV format for each guideline question, with references to the original transcript provided in JSON format for traceability.

### Pros, Cons, and Justifications

::: {=latex}
\begin{table}[htbp]
\centering
\renewcommand{\arraystretch}{1.2}
\begin{tabular}{p{3.2cm} p{4.2cm} p{4.2cm} p{5cm}}
\textbf{Component} & \textbf{Pros} & \textbf{Cons} & \textbf{Justification} \\
\hline
RAG & Combines LLM and retrieval for relevance; scales to large data & LLM-dependent; computationally complex & Outperforms standalone LLM or retrieval; scalable vs. manual methods \\
\hline
Embeddings & Captures semantic meaning; handles diverse text efficiently & Computationally expensive; model-dependent & Chosen over TF-IDF; avoids custom training using \texttt{SentenceTransformer} \\
\hline
Cosine Similarity & Normalized metric; efficient for high dimensions & Threshold tuning needed; precision-sensitive & Better than Euclidean distance; enables automated alignment \\
\hline
Referencing & Enables traceability; improves interpretability & Risk of loose matches; threshold tuning needed & More flexible than exact match; \texttt{rapidfuzz} selected for speed and integration \\
\hline
\end{tabular}
\caption{Summary of Pros, Cons, and Justifications for Core Techniques}
\end{table}
:::


### Potential Improvements and Challenges

::: {=latex}
\begin{table}[htbp]
\centering
\renewcommand{\arraystretch}{1.2}
\begin{tabular}{|p{4cm}|p{5cm}|p{5cm}|}
\hline
\textbf{Improvement} & \textbf{Challenges} & \textbf{Why Not Implemented} \\
\hline
Adaptive Thresholding & Requires extensive testing across diverse data & Fixed thresholds sufficed for current scope \\
\hline
Fine-Tuned Embeddings & Needs labeled data and high computational power & Pre-trained models met project needs within constraints \\
\hline
\end{tabular}
\caption{Unimplemented Improvements and Justifications}
\end{table}
:::

### Using the D2D Processor

The D2D processor is accessible via the `D2DProcessor` Python class, imported from the `d2d` module. Partners can use it to convert interview transcripts into structured data by initializing the processor and calling its `process_transcripts` method with paths to transcripts, guideline questions, and an output directory.

\subsubsection*{\large Key Parameters}
- **`llm_model`**: Chooses the Large Language Model (e.g., 'gpt-4o-mini' default, or other OpenAI/Anthropic models).
- **`embedding_model`**: Selects the embedding model (e.g., 'multi-qa-mpnet-base-dot-v1').
- **`sampling_method`**: Determines the sampling approach (`TOP_K` for top k matches or `TOP_P` for nucleus sampling).
- **`custom_extract_prompt`** and **`custom_summarize_prompt`**: Optional custom prompts for tailored extraction and summarization.

\subsubsection*{\large Usage Example}
{{< include _data_product_usage_example.qmd >}}

\subsubsection*{\large Installation}
The package can be installed via `pip install .` or as an installable package from the repository.

\subsubsection*{\large Additional Notes}
- The processor requires an internet connection to download the embedding model and access the LLM API.
- It includes a thematic alignment check, prompting users if transcript similarity falls below the threshold.
- The API calls to the LLM in the preprocessor and generator are asynchronous to increase the speed. For example, handling 100 transcripts can be reduced from approximately 1 hour to less than 10 minutes.

For more detailed technical information, see the [D2D repository](https://github.com/avalanche-strategy/D2D).