
The current release delivers practical business use. But as any project, new versions will be released with improved features and more flexibility.

Below are the recommended improvements that can be developed in the next few releases by the partner:

- **Integration Testing**: so far our evaluation has been restricted to using the evaluator and our personal judgement. The first and most obvious next step would be to test the data product in the end-to-end pipeline with the sample data that they had provided
- If you visualize the heatmap of the how Question-Answer Pairs match up to interview guideline questions based on pairwise cosine similarity, you will see a perfectly linear pattern (along the diagonal from top-left to bottom right) in the synthetic dataset in @fig-heatmap-food-similarity (_In the visualizations, dialogue questions are plotted on the Y-axis with the first question at the top, whereas the guideline questions are plotted left to right on the X-axis. Cells represent the cosine similarity between an interview question and the corresponding guideline question, with the darker shade representing higher similarity._). Generating realistic dialogue was challenging, and the "perfectly linear" nature of the similarity matches shows this. Interview dialogues follows the guideline almost perfectly - question after question. Nonetheless, the heatmap generated from one of the partner's actual datasets show that our pipeline is robust enough and finds the expected matches when the interview length and format varies (@fig-heatmap-1090-similarity). Despite this, even with the real-world data, we still see the diagonal pattern as much as it is less pronounced. The other area of testing for robustness would be to process some **non-sequential interviews** whose flow does not follow the guideline questions
- Still referring to @fig-heatmap-1090-similarity, we notice that in most interviews, there are some dialogue sections that have very low similarity matches to all of the guideline questions. A more precise snapshot is highlighted in @fig-heatmap-1090-similarity-post for one interview, with such sections marked with dotted lines. To improve the current process, a **post-processing step** for the matches can be added. The first objective of this step would be to fill in the gaps to any previously un-answered guideline question. For example, in @fig-heatmap-1090-similarity-post Guideline Question 2 (second column) had "[No relevant response found]" output due to the low cosine similarity scores. However, in the transcript, this question was answered by the interview dialogue pairs 2 and 3 (with the respective row numbers), given that Guideline Question 1 is answered in the first exchange (row 1) and Guideline Question 3 is answered in row 4. After this post-processing, any remaining un-used dialogue portions might be "out of topic" exchanges (for example "exit pleasantries" like _"Thank you for making time to speak with me."_ at the end [rows 16 & 17]) or they could be additional information that was not covered by the guideline questions. Our partner had set this as a stretch goal for the capstone project. Despite having limited time we had to implement it, our work has set a good stage for this objective to be achieved through this **post-processing step**

In summary, we propose additional testing with varied datasets to ascertain how well the model generalizes.


::: {layout="[[-5, 90, -5], [-5, 90, -5] ]"}

![Heatmap with QA Pairs similarity to Guidelines (Synthetic Data)](img/similarity_heatmap_food.png){#fig-heatmap-food-similarity width=50%}

![Heatmap with QA Pairs similarity to Guidelines (Real Data)](img/similarity_heatmap_1090.png){#fig-heatmap-1090-similarity}

:::

::: {layout="[[-5, 50, -45]]"}

![Heatmap showing Unused QA Pairs](img/similarity_heatmap_1090_post.png){#fig-heatmap-1090-similarity-post width=60%}

:::