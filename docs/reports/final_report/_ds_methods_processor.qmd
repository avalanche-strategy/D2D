
::: {.cell}
![RAG Workflow for D2D Processor Pipeline](img/rag_pipeline.png){#fig-rag-pipeline}
:::

### Retrieval-Augmented Generation (RAG)

RAG integrates large language model (LLM) capabilities with retrieval mechanisms to enhance the processing of interview transcripts. It leverages LLMs (e.g., ChatGPT-4o-mini[@openai2024gpt4omini] or Claude-3.5[@anthropic2024claude35sonnet]) to summarize and refine content, combined with a retrieval step to align this content with relevant guideline questions, forming the foundation for structured output generation.

### Embeddings

Embeddings convert textual data into numerical vectors using pre-trained models such as `SentenceTransformer` with `multi-qa-mpnet-base-dot-v1`[@reimers2019sentencebert]. This process captures semantic relationships within the text, enabling meaningful comparisons and matches.

### Cosine Similarity

Cosine similarity measures the cosine of the angle between two vector embeddings, providing a metric to assess the semantic similarity between summarized transcript content and guideline questions. This mathematical approach ensures effective alignment of related text.

### Referencing Process

The referencing process employs fuzzy matching with `rapidfuzz`[@bachmann2020rapidfuzz] to trace summarized answers back to their original transcript lines. This method accounts for variations in wording, ensuring outputs are linked to source data for validation and interpretability.