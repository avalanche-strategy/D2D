
The core methods include Retrieval-Augmented Generation (RAG) for transcript processing and semantic matching, semantic embeddings, cosine similarity, and a referencing process to ensure accurate and traceable outputs, complemented by a customized evaluation framework. The evaluation framework, inspired by RAGAS(Retrieval-Augmented Generation Assessment)[@es2023ragas] and Fathom’s Daedalus v4[@dsdaedalus2025], employs LLM-based prompting to compute five metrics—Correctness, Faithfulness, Precision, Recall, and Relevance—enhanced with tailored prompts, feedback mechanisms, and flexible scoring, validated using a diverse golden sample set across ten topics.

