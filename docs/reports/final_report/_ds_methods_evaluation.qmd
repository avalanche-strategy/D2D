
To assess the quality of D2Dâ€™s output, we have designed the evaluator to ensure that the answers are not only accurate in meaning, but also generated through a reliable and precise process. To support this, the evaluation framework provides five core metrics:

- **Correctness**: Measures how well the answer is consistent with the reference (ground truth).
- **Faithfulness**: Evaluates whether the answer is fully supported by the retrieved context and avoids hallucinations.
- **Precision**: Assesses the proportion of the answer that is actually supported by the retrieved chunks.
- **Recall**: Captures how many relevant facts from the context are included in the answer.
- **Relevance**: Reflects how closely the answer relates to the original guideline question. This metric is used only to assess questionnaire quality, not processor performance.

These metrics are computed using LLM-based prompting, with carefully designed templates and decision logic for edge cases such as ambiguous or evasive responses. Compared to the standard RAGAS pipeline, our customized evaluator introduces three key enhancements:

Prompt-level optimization: Tailored LLM prompts handle edge cases like vague answers. Embedding-based checks against confused templates ensure fairness when both prediction and ground truth lack specificity.

Integrated feedback generation: Each score includes LLM-generated feedback, which improves interpretability and helps users understand and verify the results.

Configurable scoring and thresholding: The evaluator supports customizable metric weights and threshold-based flagging of low-scoring responses, streamlining validation workflows and adapting to diverse evaluation requirements.
