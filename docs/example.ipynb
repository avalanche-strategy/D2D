{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d051c1d7dff8e04e",
   "metadata": {},
   "source": [
    "# Dialogue2Data (D2D) Package Documentation\n",
    "\n",
    "Welcome to the documentation for the `d2d` package! Dialogue2Data (D2D) is a Python-based, open-source tool designed to transform unstructured interview transcripts into structured data. By leveraging Natural Language Processing (NLP) and Large Language Models (LLMs), D2D automates the extraction, matching, and summarization of responses based on predefined guideline questions. This package is ideal for researchers, analysts, and organizations aiming to derive insights from qualitative data efficiently.\n",
    "\n",
    "## Installation\n",
    "1. Clone the repository:\n",
    "```bash\n",
    "git clone https://github.com/fathomthat/d2d.git\n",
    "cd D2D\n",
    "```\n",
    "2. Create and activate the Conda environment:\n",
    "```bash\n",
    "conda env create -f environment.yml\n",
    "conda activate d2d\n",
    "```\n",
    "\n",
    "## Environment Configuration\n",
    "To use the OpenAI and Anthropic API, you need to set up an environment variable for your API key. Create a `.env` file in the root directory of the project with the following content:\n",
    "\n",
    "```bash\n",
    "OPENAI_API_KEY=[Please replace this with your OPENAI API key]\n",
    "ANTHROPIC_API_KEY=[Please replace this with your OPENAI API key]\n",
    "```\n",
    "\n",
    "### Data Placement\n",
    "\n",
    "To ensure smooth operation of the processor, please organize your data as follows:\n",
    "\n",
    "- **Interview Data Structure**:\n",
    "  Each interview should have its own subdirectory. The name of this subdirectory is the **interview name** (e.g., `interview_1090`). While it is suggested to place these directories under `data/private_data/` for confidentiality, you may choose a different location if needed.\n",
    "\n",
    "- **Transcript Files**:\n",
    "  Place all transcript files directly inside the interview directory. For example:\n",
    "  - `data/private_data/interview_1090/transcript.txt`\n",
    "\n",
    "- **Guidelines CSV File**:\n",
    "  The guidelines CSV file must be placed in the same level of the interview directory and should be named consistently with the interview name. For example:\n",
    "  - `data/private_data/interview_1090_guidelines.csv`\n",
    "\n",
    "- **Synthetic Data**:\n",
    "  For demonstration purposes, synthetic data is provided in `data/synthetic_data/interview_food`. This can be used to test the processor without needing private data.\n",
    "\n",
    "## How It Works (Processor part)\n",
    "\n",
    "The processor follows these steps:\n",
    "1. **Segmentation**: Divides the transcript into question-response pairs.\n",
    "2. **Summarization**: Summarize the questions in the transcript and guideline questions.\n",
    "3. **Embedding**: Uses a SentenceTransformer model to embed summarized questions in the transcript and guideline questions.\n",
    "4. **Matching**: Matches segments to guideline questions via cosine similarity.\n",
    "5. **Summarization**: Summarizes matched segments using an LLM.\n",
    "6. **Output**: Generates a CSV with summaries, plus JSON metadata and a log file.\n",
    "\n",
    "## Usage\n",
    "\n",
    "To run the processor on the synthetic data, use the following command after setting up your environment and data:\n",
    "\n",
    "```bash\n",
    "python examples/api_test/processor_test.py\n",
    "```\n",
    "\n",
    "**Note: for more detailed and executable examples, please refer to `examples/api_test/processor_test.py`. The following example doesn't work until the package is pushed to PyPI**\n",
    "\n",
    "### Example\n",
    "Here’s how to process a sample transcript:\n",
    "\n",
    "```python\n",
    "# Step 0: Import the processor correctly.\n",
    "from d2d.processor import D2DProcessor\n",
    "\n",
    "# Suppose You have correctly imported the processor\n",
    "# Step 1: Initialize the processor\n",
    "processor = D2DProcessor(\n",
    "    llm_model=\"gpt-4o-mini\",\n",
    "    embedding_model=\"multi-qa-mpnet-base-dot-v1\",\n",
    "    max_concurrent_calls=10,\n",
    "    sampling_method=D2DProcessor.SamplingMethod.TOP_K,\n",
    ")\n",
    "\n",
    "# Step 2: Define paths relative to the root directory\n",
    "root_dir = os.path.abspath(os.path.join(os.path.dirname(__file__), '../..'))\n",
    "data_dir = os.path.join(root_dir, \"data\", \"synthetic_data\")\n",
    "interview_name = \"interview_food\"\n",
    "output_dir = os.path.join(root_dir, \"results\")\n",
    "\n",
    "# Step 3: Start transcripts processing\n",
    "processor.process_transcripts(\n",
    "    data_dir=data_dir,\n",
    "    interview_name=interview_name,\n",
    "    output_dir=output_dir,\n",
    "    disable_logging=False\n",
    "    )\n",
    "# Process completed\n",
    "```\n",
    "\n",
    "\n",
    "\n",
    "## Parameters\n",
    "\n",
    "### Initialization Parameters `D2DProcessor`\n",
    "**Note that you don't have to set any parameters, each of them has default setting. You can simply run `processor = D2DProcessor()`**\n",
    "\n",
    "- **llm_model : str**  \n",
    "  The Large Language Model to use for summarization (e.g., \"gpt-4o-mini\"). This specifies the model that processes the text data.\n",
    "\n",
    "- **embedding_model : str**  \n",
    "  The SentenceTransformer model used for embedding transcript segments and guideline questions (e.g., \"multi-qa-mpnet-base-dot-v1\"). This model converts text into vector representations for similarity matching.\n",
    "\n",
    "- **max_concurrent_calls : int**  \n",
    "  Maximum number of concurrent API calls to the LLM, enabling efficient parallel processing of transcript segments.\n",
    "\n",
    "- **sampling_method : D2DProcessor.SamplingMethod**  \n",
    "  Determines the method for sampling matched segments from the transcript:\n",
    "  - `TOP_K`: Selects the top K segments based on similarity scores.\n",
    "  - `TOP_P`: Selects segments until the cumulative probability reaches a specified threshold P.\n",
    "\n",
    "- **top_k : int, optional**  \n",
    "  Number of top segments to consider when using `TOP_K` sampling. Default is 5. Ignored if `sampling_method` is `TOP_P`.\n",
    "\n",
    "- **top_p : float, optional**  \n",
    "  Cumulative probability threshold for `TOP_P` sampling. Default is 0.5. Ignored if `sampling_method` is `TOP_K`.\n",
    "\n",
    "- **custom_extract_prompt : str, optional**  \n",
    "  Custom prompt template for extracting key phrases from matched segments. The template can include `{context}` (the transcript segment) and `{query}` (the guideline question). If not provided, a default extraction prompt is used. Example:  \n",
    "  `\"Using the dialogue: {context}, find a short phrase from the interviewee that answers '{query}'. Avoid pronouns and use explicit names. If no answer is found, return '[No answer found]'.\"`\n",
    "\n",
    "- **custom_summarize_prompt : str, optional**  \n",
    "  Custom prompt template for summarizing extracted phrases. The template can include `{extracted_phrase}` (the extracted text) and `{query}` (the guideline question). If not provided, a default summarization prompt is used. Example:  \n",
    "  `\"From the phrase: {extracted_phrase}, for the query '{query}', create a brief summary using only the original words, focusing on the main point.\"`\n",
    "\n",
    "### Processing Parameters `process_transcripts`\n",
    "\n",
    "- **data_dir : str**  \n",
    "  Directory containing the interview data, including transcript files and a guideline CSV file. This is the root path for input data.\n",
    "\n",
    "- **interview_name : str**  \n",
    "  Name of the interview folder within `data_dir`. Specifies which interview’s data to process.\n",
    "\n",
    "- **output_dir : str**  \n",
    "  Directory where output files (e.g., CSV, JSON, log files) will be saved. This is the destination for processed results.\n",
    "\n",
    "- **disable_logging : bool, optional**  \n",
    "  If set to `True`, disables logging to both the console and log file. Default is `False`, enabling logging for debugging and tracking purposes.\n",
    "\n",
    "\n",
    "## Additional Notes\n",
    "\n",
    "1. **Edge Cases**:  \n",
    "   - Empty transcripts or missing guidelines will raise errors.  \n",
    "   - Ensure API keys are set for LLM access.\n",
    "\n",
    "2. **Performance**:  \n",
    "   - Processing time depends on transcript length and question count. Use GPU for faster computation.\n",
    "\n",
    "## Summary\n",
    "\n",
    "The `d2d` package simplifies the transformation of interview transcripts into structured data, offering:\n",
    "\n",
    "- **Automated Segmentation** and matching to guideline questions.\n",
    "- **LLM-Powered Summarization** for concise insights.\n",
    "- **Structured Outputs** for easy analysis.\n",
    "\n",
    "Explore D2D for your qualitative data needs!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0dc906a5-0d28-48b8-9e18-38a5bf1ca6af",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-26T03:04:34.349418Z",
     "start_time": "2025-05-26T03:04:34.346576Z"
    }
   },
   "source": [
    "The following sections will replace the upper part when the package is published "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5dfcc50-eb2e-44af-a702-57752d488b74",
   "metadata": {},
   "source": [
    "## Installation\n",
    "To start using the `d2d` package, install it via pip in your terminal:\n",
    "```bash\n",
    "pip install d2d\n",
    "```"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
