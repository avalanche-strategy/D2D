{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d051c1d7dff8e04e",
   "metadata": {},
   "source": [
    "# Dialogue2Data (D2D) Package Documentation\n",
    "\n",
    "Welcome to the documentation for the `d2d` package! Dialogue2Data (D2D) is a Python-based, open-source tool designed to transform unstructured interview transcripts into structured data. By leveraging Natural Language Processing (NLP) and Large Language Models (LLMs), D2D automates the extraction, matching, and summarization of responses based on predefined guideline questions. This package is ideal for researchers, analysts, and organizations aiming to derive insights from qualitative data efficiently.\n",
    "\n",
    "---\n",
    "\n",
    "## Installation\n",
    "1. Clone the repository:\n",
    "```bash\n",
    "git clone https://github.com/avalanche-strategy/D2D.git\n",
    "cd D2D\n",
    "```\n",
    "2. Create and activate the Conda environment:\n",
    "```bash\n",
    "conda env create -f environment.yml\n",
    "conda activate d2d\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## Environment Configuration\n",
    "To use the OpenAI and Anthropic API, you need to set up an environment variable for your API key. Create a `.env` file in the root directory of the project with the following content:\n",
    "\n",
    "- **Example:**  \n",
    "```bash\n",
    "OPENAI_API_KEY=sk-abc123XYZ789pqr456STU012vwx789YZ\n",
    "ANTHROPIC_API_KEY=sk-ant-987ZYX654WVU321TSR098qwe456PLM\n",
    "```\n",
    "\n",
    "**Note: This are fictional keys.**\n",
    "\n",
    "---\n",
    "\n",
    "### Data Placement\n",
    "\n",
    "To ensure smooth operation, please organize your data as follows:\n",
    "\n",
    "- **Interview Data Structure (for processor)**:\n",
    "  Each interview should have its own subdirectory. The name of this subdirectory is the **interview name**, which should be in the format `interview_XXXX`, where `XXXX` is a unique identifier for the interview (e.g., `interview_food` is a folder containing interview transcript files for food theme.). While it is suggested to place these directories under `data/private_data/` for confidentiality, you may choose a different location if needed.\n",
    "\n",
    "- **Transcript TXT Files (for processor)**:\n",
    "  There is no requirement for the naming of the transcript files. Just make sure all transcript files are placed directly inside the interview directory. For example:\n",
    "  - `data/private_data/interview_food/transcript1.txt`\n",
    "  - `data/private_data/interview_food/transcript2.txt`\n",
    "  - etc.  \n",
    "\n",
    "- **Guidelines CSV File (for processor)**:\n",
    "  A CSV file named `interview_xxx_guidelines.csv` containing the guideline questions. There should be a column named `guide_text` with the guideline questions. For example:\n",
    "  - `interview_food_guidelines.csv` contains the guideline questions for the interviews of food theme.\n",
    "\n",
    "- **Reference Answer (for evaluation)**: A CSV file named `response_xxx.csv` containing the reference answers for the guideline questions. The first column should be `respondent_id`, and the remaining columns should be the reference answers to the corresponding guideline questions. For example:\n",
    "  - `response_food.csv` contains the reference answers for the food theme.\n",
    "\n",
    "---\n",
    "\n",
    "## Data Format and Sample Data Output for D2D Pipeline\n",
    "\n",
    "### Data Input\n",
    "The D2D pipeline (processor part) processes two types of input files to extract and structure responses from unstructured interview transcripts based on provided guidelines.\n",
    "\n",
    "### 1. Guidelines\n",
    "- **Description**: A structured file listing the questions or prompts to guide the interview and extraction process.\n",
    "- **Format**: Comma-separated values (`.csv`)\n",
    "- **Structure**:\n",
    "  - Single column named `guide_text` with each row containing a question or prompt.\n",
    "  - Questions align with those asked in transcripts for matching purposes.\n",
    "- **Example**:\n",
    "  - **File**: `interview_food_sample_guidelines.csv`\n",
    "    > `guide_text`  \n",
    "    > What’s a dish that reminds you of your childhood?  \n",
    "    > Can you describe a meal that has a special meaning for you?\n",
    "    > ...\n",
    "\n",
    "### 2. Transcripts\n",
    "- **Description**: Raw text files containing conversational interview data, with alternating lines or labeled segments for interviewers and interviewees.\n",
    "- **Format**: Plain text (`.txt`)\n",
    "- **Structure**:\n",
    "  - Each file represents one interview, named sequentially (e.g., `001.txt`, `002.txt`).\n",
    "  - Content includes dialogue, with questions from interviewers and responses from interviewees.\n",
    "- **Example**:\n",
    "  - **File**: `001.txt`\n",
    "    > Interviewer: Let’s talk food. What’s a dish that reminds you of your childhood?  \n",
    "    > Interviewee: Definitely my grandma’s chicken and rice. She used to make it every Sunday, and the smell would just take over the whole house. It was simple—nothing fancy—but it was filled with love.  \n",
    "    > Interviewer: Can you describe a meal that has a special meaning for you?  \n",
    "    > Interviewee: Yeah, actually. My 18th birthday dinner. My parents surprised me by cooking all my favorite dishes—pad thai, roasted veggies, and this chocolate lava cake I was obsessed with. I remember feeling really seen, you know?\n",
    "    > ...\n",
    "  - **File**: `002.txt`\n",
    "    > Interviewer: Alright, diving into food and memories—what dish instantly brings your childhood back?  \n",
    "    > Interviewee: Oh man, my mom’s arroz con leche. She’d make it every time I was sick, or honestly, just when I needed cheering up. The cinnamon smell still makes me emotional sometimes.  \n",
    "    > Interviewer: Can you describe a meal that holds special meaning for you?  \n",
    "    > Interviewee: Our Christmas Eve dinner. It’s this big spread—tamales, roasted pork, rice, beans. It’s loud and chaotic and full of stories. It’s more than food—it’s our whole culture on a table.\n",
    "    > ...\n",
    "\n",
    "\n",
    "\n",
    "### Sample Data Output\n",
    "The D2D pipeline produces structured output by matching interviewee responses to guideline questions, consolidating results for analysis.\n",
    "- **Format**: Comma-separated values (`.csv`)\n",
    "- **Structure**:\n",
    "  - Columns:\n",
    "    - `Interview File`: Identifier of the source transcript file (e.g., `001`, `002`).\n",
    "    - Additional columns named after guideline questions (e.g., \"What’s a dish that reminds you of your childhood?\").\n",
    "  - Each row corresponds to one interview, with cells containing the extracted response text.\n",
    "  - Responses are concise, summarizing key points from the transcript.  \n",
    "- **Example**:\n",
    "| Interview File | What’s a dish that reminds you of your childhood? | Can you describe a meal that has a special meaning for you?                          |...|\n",
    "|----------------|--------------------------------------------------|------------------------------------------------------------------------------------|---|\n",
    "| 001            | Grandma’s chicken and rice                       | 18th birthday dinner with favorite dishes cooked by parents.                        |...|\n",
    "| 002            | Mom’s arroz con leche                            | Christmas Eve dinner with tamales, roasted pork, rice, beans; loud, chaotic, full of stories. |...|\n",
    "|...|...|...|...|\n",
    "\n",
    "---\n",
    "\n",
    "## How It Works (Processor part)\n",
    "\n",
    "The processor follows these steps:\n",
    "1. **Segmentation**: Divides the transcript into question-response pairs.\n",
    "2. **Summarization**: Summarize the questions in the transcript and guideline questions.\n",
    "3. **Embedding**: Uses a SentenceTransformer model to embed summarized questions in the transcript and guideline questions.\n",
    "4. **Matching**: Matches segments to guideline questions via cosine similarity.\n",
    "5. **Summarization**: Summarizes matched segments using an LLM.\n",
    "6. **Output**: Generates a CSV with summaries, plus JSON metadata and a log file.\n",
    "\n",
    "---\n",
    "\n",
    "## Usage\n",
    "\n",
    "To run the processor on the synthetic data, use the following command after setting up your environment and data:\n",
    "\n",
    "```bash\n",
    "python examples/api_test/processor_test.py\n",
    "```\n",
    "---\n",
    "\n",
    "## Data Output\n",
    "\n",
    "Upon processing, the D2D pipeline generates four output files, which are stored in a user-designated directory. In the examples, this directory is `results/`, but users can specify any preferred path when running the processor. For the synthetic interview dataset `interview_food`, processed at 11:13 AM PDT on May 26, 2025, the following files are generated:\n",
    "\n",
    "- **`D2D_survey_food_generator_log_202505261113.txt`**\n",
    "  - **Description**: This plain text file contains the retrieval information passed into the generator module for the `interview_food` dataset, specifically for use in the evaluation process. It logs the data or queries that the generator utilizes to produce synthetic or processed outputs.\n",
    "  - **Purpose**: It supports the evaluation module by providing transparency into the inputs fed to the generator, aiding in assessing its performance or accuracy.\n",
    "\n",
    "- **`D2D_survey_food_log_202505261113.log`**\n",
    "  - **Description**: This log file records the processing steps applied to the `interview_food` dataset within the D2D pipeline. It captures details such as segmentation, embedding, matching, and summarization, along with any informational messages, warnings, or errors encountered during execution.\n",
    "  - **Purpose**: It acts as a comprehensive record of the workflow, facilitating debugging and ensuring that each processing step is documented for review or troubleshooting.\n",
    "\n",
    "- **`D2D_survey_food_references_202505261113.json`**\n",
    "  - **Description**: This JSON file provides reference information for the `interview_food` dataset, specifying the exact lines in the transcripts from which responses were extracted for each interview. It serves as a mapping to link summarized responses back to their original context within the source transcripts.\n",
    "  - **Purpose**: It enables users to trace and verify the origins of the extracted responses, offering insight into the context and accuracy of the summarization process.\n",
    "\n",
    "- **`D2D_survey_food_responses_202505261113.csv`**\n",
    "  - **Description**: This CSV file delivers the structured output of the D2D pipeline for the `interview_food` dataset. Each row typically corresponds to a single transcript, with columns containing summarized responses aligned with the guideline questions from the interview guidelines.\n",
    "  - **Purpose**: It provides a concise, tabular summary of key insights from the `interview_food` transcripts, making the data readily accessible for analysis or further use.\n",
    "\n",
    "**Note**: The timestamp in the file names (e.g., `202505261113`) is based on the date and time when the processing was run, ensuring unique file names for each execution and preventing overwrites.\n",
    "\n",
    "---\n",
    "\n",
    "## Explanation of the API\n",
    "**Note: for more detailed and executable examples, please refer to `examples/api_test/processor_test.py`. The first line of following example doesn't work until the package is pushed to PyPI**  \n",
    "\n",
    "### API Example\n",
    "Here’s how to process a sample transcript:\n",
    "\n",
    "```python\n",
    "# Step 0: Import the processor correctly.\n",
    "from d2d.processor import D2DProcessor\n",
    "\n",
    "# Suppose You have correctly imported the processor\n",
    "# Step 1: Initialize the processor\n",
    "processor = D2DProcessor(\n",
    "    llm_model=\"gpt-4o-mini\",\n",
    "    embedding_model=\"multi-qa-mpnet-base-dot-v1\",\n",
    "    max_concurrent_calls=10,\n",
    "    sampling_method=D2DProcessor.SamplingMethod.TOP_K,\n",
    ")\n",
    "\n",
    "# Step 2: Define paths relative to the root directory\n",
    "root_dir = os.path.abspath(os.path.join(os.path.dirname(__file__), '../..'))\n",
    "data_dir = os.path.join(root_dir, \"data\", \"synthetic_data\")\n",
    "interview_name = \"interview_food\"\n",
    "output_dir = os.path.join(root_dir, \"results\")\n",
    "\n",
    "# Step 3: Start transcripts processing\n",
    "processor.process_transcripts(\n",
    "    data_dir=data_dir,\n",
    "    interview_name=interview_name,\n",
    "    output_dir=output_dir,\n",
    "    disable_logging=False\n",
    "    )\n",
    "# Process completed\n",
    "```\n",
    "\n",
    "### Parameters\n",
    "\n",
    "#### Initialization Parameters `D2DProcessor`\n",
    "**Note that you don't have to set any parameters, each of them has a default setting. You can simply run `processor = D2DProcessor()`**\n",
    "\n",
    "- **llm_model : str**\n",
    "  The Large Language Model to use for summarization (e.g., \"gpt-4o-mini\"). This specifies the model that processes the text data.\n",
    "\n",
    "- **embedding_model : str**\n",
    "  The SentenceTransformer model used for embedding transcript segments and guideline questions (e.g., \"multi-qa-mpnet-base-dot-v1\"). This model converts text into vector representations for similarity matching.\n",
    "\n",
    "- **max_concurrent_calls : int**\n",
    "  Maximum number of concurrent API calls to the LLM, enabling efficient parallel processing of transcript segments.\n",
    "\n",
    "- **sampling_method : D2DProcessor.SamplingMethod**\n",
    "  Determines the method for sampling matched segments from the transcript:\n",
    "  - `TOP_K`: Selects the top K segments based on similarity scores.\n",
    "  - `TOP_P`: Selects segments until the cumulative probability reaches a specified threshold P.\n",
    "\n",
    "- **top_k : int, optional**\n",
    "  Number of top segments to consider when using `TOP_K` sampling. Default is 5. Ignored if `sampling_method` is `TOP_P`.\n",
    "\n",
    "- **top_p : float, optional**\n",
    "  Cumulative probability threshold for `TOP_P` sampling. Default is 0.5. Ignored if `sampling_method` is `TOP_K`.\n",
    "\n",
    "- **custom_extract_prompt : str, optional**\n",
    "  Custom prompt template for extracting key phrases from matched segments. The template can include `{context}` (the transcript segment) and `{query}` (the guideline question). If not provided, a default extraction prompt is used. Example:\n",
    "  `\"Using the dialogue: {context}, find a short phrase from the interviewee that answers '{query}'. Avoid pronouns and use explicit names. If no answer is found, return '[No answer found]'.\"`\n",
    "\n",
    "- **custom_summarize_prompt : str, optional**\n",
    "  Custom prompt template for summarizing extracted phrases. The template can include `{extracted_phrase}` (the extracted text) and `{query}` (the guideline question). If not provided, a default summarization prompt is used. Example:\n",
    "  `\"From the phrase: {extracted_phrase}, for the query '{query}', create a brief summary using only the original words, focusing on the main point.\"`\n",
    "\n",
    "#### Processing Parameters `process_transcripts`\n",
    "\n",
    "- **data_dir : str**\n",
    "  Directory containing the interview data, including transcript files and a guideline CSV file. This is the root path for input data.\n",
    "\n",
    "- **interview_name : str**\n",
    "  Name of the interview folder within `data_dir`. Specifies which interview’s data to process.\n",
    "\n",
    "- **output_dir : str**\n",
    "  Directory where output files (e.g., CSV, JSON, log files) will be saved. This is the destination for processed results.\n",
    "\n",
    "- **disable_logging : bool, optional**\n",
    "  If set to `True`, disables logging to both the console and log file. Default is `False`, enabling logging for debugging and tracking purposes.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0ad2605-d939-4e7f-a84f-4c65f3606b2f",
   "metadata": {},
   "source": [
    "## Evaluator\n",
    "---\n",
    "\n",
    "## Data Format and Sample Data Output\n",
    "\n",
    "### Data Input\n",
    "The D2D pipeline (evaluator part) evaluates the performance of the processor and scores the pipeline result with 5 metrics and a weighted join score. \n",
    "The evaluator takes 2 outputs of the processor and a reference answer as inputs.\n",
    "\n",
    "### 1. CSV Output from Processor\n",
    "- **Description**: A CSV file of the structured output of the D2D pipeline for the given dataset. Each row typically corresponds to a single transcript,\n",
    "  with columns containing summarized responses aligned with the guideline questions from the interview guidelines.\n",
    "- **Format**: Comma-separated values (`.csv`)\n",
    "- **Structure**:\n",
    "  - Columns:\n",
    "    - `Interview File`: Identifier of the source transcript file (e.g., `001`, `002`).\n",
    "    - Additional columns named after guideline questions (e.g., \"What’s a dish that reminds you of your childhood?\").\n",
    "  - Each row corresponds to one interview, with cells containing the extracted response text.\n",
    "  - Responses are concise, summarizing key points from the transcript.  \n",
    "- **Example**:\n",
    "| Interview File | What’s a dish that reminds you of your childhood? | Can you describe a meal that has a special meaning for you?                          |...|\n",
    "|----------------|--------------------------------------------------|------------------------------------------------------------------------------------|---|\n",
    "| 001            | Grandma’s chicken and rice                       | 18th birthday dinner with favorite dishes cooked by parents.                        |...|\n",
    "| 002            | Mom’s arroz con leche                            | Christmas Eve dinner with tamales, roasted pork, rice, beans; loud, chaotic, full of stories. |...|\n",
    "|...|...|...|...|\n",
    "\n",
    "### 2. Log (TXT) Output from Processor\n",
    "- **Description**: The log file records the processing steps applied to the given interview dataset within the D2D pipeline.\n",
    "  It captures details such as segmentation, embedding, matching, and summarization, along with any informational messages, warnings,\n",
    "  or errors encountered during execution.\n",
    "- **Format**: Plain text (`.txt`)\n",
    "- **Structure**:\n",
    "  - Each chunk of text marked with `===Start===` and `===End===` includes one analyzed guideline question from a single interview file.\n",
    "  - Each chunk is consisted of the file name, guideline question, and relevant chunks of questions and answers that may contain the response. \n",
    "- **Example**:\n",
    "  > ===Start===  \n",
    "  > Processing file: 002  \n",
    "  > Processing guide question: How do food and family traditions connect for you?  \n",
    "  > Relevant Interviewee Responses:  \n",
    "  > Interviewer: What about food and family traditions—how do they connect?  \n",
    "  > Interviewee: They’re basically the same thing in my family. Recipes are sacred. Like, if you try to tweak my aunt’s flan recipe, you might start a family feud. [laughs]  \n",
    "  > Interviewer: Any food tied to a place or person for you?  \n",
    "  > Interviewee: Yeah—empanadas always remind me of my grandma in Buenos Aires. She’d let me help fold the dough, and I’d sneak bits of the filling when she wasn’t looking.  \n",
    "  > Interviewer: Favorite dish from another culture?  \n",
    "  > Interviewee: Japanese ramen. The broth, the noodles, the toppings—it’s like a bowl of magic. I tried making it once. Total disaster. [laughs]  \n",
    "  > Interviewer: Alright, diving into food and memories—what dish instantly brings your childhood back?  \n",
    "  > Interviewee: Oh man, my mom’s arroz con leche. She’d make it every time I was sick, or honestly, just when I needed cheering up. The cinnamon smell still makes me emotional sometimes.  \n",
    "  > Interviewer: What’s the first thing you learned to cook?  \n",
    "  > Interviewee: French toast! I was like nine, and I made it for my dad on Father’s Day. I used way too much cinnamon, but he ate it like it was gourmet. I’ll never forget that.  \n",
    "  > ===End===\n",
    "  \n",
    "### 3. \n",
    "\n",
    "---\n",
    "\n",
    "## How It Works (Processor part)\n",
    "\n",
    "The processor follows these steps:\n",
    "1. **Segmentation**: Divides the transcript into question-response pairs.\n",
    "2. **Summarization**: Summarize the questions in the transcript and guideline questions.\n",
    "3. **Embedding**: Uses a SentenceTransformer model to embed summarized questions in the transcript and guideline questions.\n",
    "4. **Matching**: Matches segments to guideline questions via cosine similarity.\n",
    "5. **Summarization**: Summarizes matched segments using an LLM.\n",
    "6. **Output**: Generates a CSV with summaries, plus JSON metadata and a log file.\n",
    "\n",
    "---\n",
    "\n",
    "## Usage\n",
    "\n",
    "To run the processor on the synthetic data, use the following command after setting up your environment and data:\n",
    "\n",
    "```bash\n",
    "python examples/api_test/processor_test.py\n",
    "```\n",
    "---\n",
    "\n",
    "## Data Output\n",
    "\n",
    "Upon processing, the D2D pipeline generates four output files, which are stored in a user-designated directory. In the examples, this directory is `results/`, but users can specify any preferred path when running the processor. For the synthetic interview dataset `interview_food`, processed at 11:13 AM PDT on May 26, 2025, the following files are generated:\n",
    "\n",
    "- **`D2D_survey_food_generator_log_202505261113.txt`**\n",
    "  - **Description**: This plain text file contains the retrieval information passed into the generator module for the `interview_food` dataset, specifically for use in the evaluation process. It logs the data or queries that the generator utilizes to produce synthetic or processed outputs.\n",
    "  - **Purpose**: It supports the evaluation module by providing transparency into the inputs fed to the generator, aiding in assessing its performance or accuracy.\n",
    "\n",
    "- **`D2D_survey_food_log_202505261113.log`**\n",
    "  - **Description**: This log file records the processing steps applied to the `interview_food` dataset within the D2D pipeline. It captures details such as segmentation, embedding, matching, and summarization, along with any informational messages, warnings, or errors encountered during execution.\n",
    "  - **Purpose**: It acts as a comprehensive record of the workflow, facilitating debugging and ensuring that each processing step is documented for review or troubleshooting.\n",
    "\n",
    "- **`D2D_survey_food_references_202505261113.json`**\n",
    "  - **Description**: This JSON file provides reference information for the `interview_food` dataset, specifying the exact lines in the transcripts from which responses were extracted for each interview. It serves as a mapping to link summarized responses back to their original context within the source transcripts.\n",
    "  - **Purpose**: It enables users to trace and verify the origins of the extracted responses, offering insight into the context and accuracy of the summarization process.\n",
    "\n",
    "- **`D2D_survey_food_responses_202505261113.csv`**\n",
    "  - **Description**: This CSV file delivers the structured output of the D2D pipeline for the `interview_food` dataset. Each row typically corresponds to a single transcript, with columns containing summarized responses aligned with the guideline questions from the interview.\n",
    "  - **Purpose**: It provides a concise, tabular summary of key insights from the `interview_food` transcripts, making the data readily accessible for analysis or further use.\n",
    "\n",
    "**Note**: The timestamp in the file names (e.g., `202505261113`) is based on the date and time when the processing was run, ensuring unique file names for each execution and preventing overwrites.\n",
    "\n",
    "---\n",
    "\n",
    "## Explanation of the API\n",
    "**Note: for more detailed and executable examples, please refer to `examples/api_test/processor_test.py`. The first line of following example doesn't work until the package is pushed to PyPI**  \n",
    "\n",
    "### API Example\n",
    "Here’s how to process a sample transcript:\n",
    "\n",
    "```python\n",
    "# Step 0: Import the processor correctly.\n",
    "from d2d.processor import D2DProcessor\n",
    "\n",
    "# Suppose You have correctly imported the processor\n",
    "# Step 1: Initialize the processor\n",
    "processor = D2DProcessor(\n",
    "    llm_model=\"gpt-4o-mini\",\n",
    "    embedding_model=\"multi-qa-mpnet-base-dot-v1\",\n",
    "    max_concurrent_calls=10,\n",
    "    sampling_method=D2DProcessor.SamplingMethod.TOP_K,\n",
    ")\n",
    "\n",
    "# Step 2: Define paths relative to the root directory\n",
    "root_dir = os.path.abspath(os.path.join(os.path.dirname(__file__), '../..'))\n",
    "data_dir = os.path.join(root_dir, \"data\", \"synthetic_data\")\n",
    "interview_name = \"interview_food\"\n",
    "output_dir = os.path.join(root_dir, \"results\")\n",
    "\n",
    "# Step 3: Start transcripts processing\n",
    "processor.process_transcripts(\n",
    "    data_dir=data_dir,\n",
    "    interview_name=interview_name,\n",
    "    output_dir=output_dir,\n",
    "    disable_logging=False\n",
    "    )\n",
    "# Process completed\n",
    "```\n",
    "\n",
    "### Parameters\n",
    "\n",
    "#### Initialization Parameters `D2DProcessor`\n",
    "**Note that you don't have to set any parameters, each of them has a default setting. You can simply run `processor = D2DProcessor()`**\n",
    "\n",
    "- **llm_model : str**\n",
    "  The Large Language Model to use for summarization (e.g., \"gpt-4o-mini\"). This specifies the model that processes the text data.\n",
    "\n",
    "- **embedding_model : str**\n",
    "  The SentenceTransformer model used for embedding transcript segments and guideline questions (e.g., \"multi-qa-mpnet-base-dot-v1\"). This model converts text into vector representations for similarity matching.\n",
    "\n",
    "- **max_concurrent_calls : int**\n",
    "  Maximum number of concurrent API calls to the LLM, enabling efficient parallel processing of transcript segments.\n",
    "\n",
    "- **sampling_method : D2DProcessor.SamplingMethod**\n",
    "  Determines the method for sampling matched segments from the transcript:\n",
    "  - `TOP_K`: Selects the top K segments based on similarity scores.\n",
    "  - `TOP_P`: Selects segments until the cumulative probability reaches a specified threshold P.\n",
    "\n",
    "- **top_k : int, optional**\n",
    "  Number of top segments to consider when using `TOP_K` sampling. Default is 5. Ignored if `sampling_method` is `TOP_P`.\n",
    "\n",
    "- **top_p : float, optional**\n",
    "  Cumulative probability threshold for `TOP_P` sampling. Default is 0.5. Ignored if `sampling_method` is `TOP_K`.\n",
    "\n",
    "- **custom_extract_prompt : str, optional**\n",
    "  Custom prompt template for extracting key phrases from matched segments. The template can include `{context}` (the transcript segment) and `{query}` (the guideline question). If not provided, a default extraction prompt is used. Example:\n",
    "  `\"Using the dialogue: {context}, find a short phrase from the interviewee that answers '{query}'. Avoid pronouns and use explicit names. If no answer is found, return '[No answer found]'.\"`\n",
    "\n",
    "- **custom_summarize_prompt : str, optional**\n",
    "  Custom prompt template for summarizing extracted phrases. The template can include `{extracted_phrase}` (the extracted text) and `{query}` (the guideline question). If not provided, a default summarization prompt is used. Example:\n",
    "  `\"From the phrase: {extracted_phrase}, for the query '{query}', create a brief summary using only the original words, focusing on the main point.\"`\n",
    "\n",
    "#### Processing Parameters `process_transcripts`\n",
    "\n",
    "- **data_dir : str**\n",
    "  Directory containing the interview data, including transcript files and a guideline CSV file. This is the root path for input data.\n",
    "\n",
    "- **interview_name : str**\n",
    "  Name of the interview folder within `data_dir`. Specifies which interview’s data to process.\n",
    "\n",
    "- **output_dir : str**\n",
    "  Directory where output files (e.g., CSV, JSON, log files) will be saved. This is the destination for processed results.\n",
    "\n",
    "- **disable_logging : bool, optional**\n",
    "  If set to `True`, disables logging to both the console and log file. Default is `False`, enabling logging for debugging and tracking purposes.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6153e97e-8cc8-462e-9b27-e87c5f94551b",
   "metadata": {},
   "source": [
    "## Additional Notes\n",
    "\n",
    "1. **Edge Cases**:\n",
    "   - Empty transcripts or missing guidelines will raise errors.\n",
    "   - Ensure API keys are set for LLM access.\n",
    "\n",
    "2. **Performance**:\n",
    "   - Processing time depends on transcript length and question count. Use GPU for faster computation.\n",
    "\n",
    "## Summary\n",
    "\n",
    "The `d2d` package simplifies the transformation of interview transcripts into structured data, offering:\n",
    "\n",
    "- **Automated Segmentation** and matching to guideline questions.\n",
    "- **LLM-Powered Summarization** for concise insights.\n",
    "- **Structured Outputs** for easy analysis.\n",
    "\n",
    "Explore D2D for your qualitative data needs!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0dc906a5-0d28-48b8-9e18-38a5bf1ca6af",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-26T03:04:34.349418Z",
     "start_time": "2025-05-26T03:04:34.346576Z"
    }
   },
   "source": [
    "**The following sections will replace the upper part when the package is published to PyPI**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5dfcc50-eb2e-44af-a702-57752d488b74",
   "metadata": {},
   "source": [
    "## Installation\n",
    "To start using the `d2d` package, install it via pip in your terminal:\n",
    "```bash\n",
    "pip install d2d\n",
    "```"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
