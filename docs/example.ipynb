{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d051c1d7dff8e04e",
   "metadata": {},
   "source": [
    "# Dialogue2Data (D2D) Package Documentation\n",
    "\n",
    "Welcome to the documentation for the `d2d` package! Dialogue2Data (D2D) is a Python-based, open-source tool designed to transform unstructured interview transcripts into structured data. By leveraging Natural Language Processing (NLP) and Large Language Models (LLMs), D2D automates the extraction, matching, and summarization of responses based on predefined guideline questions. This package is ideal for researchers, analysts, and organizations aiming to derive insights from qualitative data efficiently.\n",
    "\n",
    "---\n",
    "\n",
    "## Installation\n",
    "1. Clone the repository:\n",
    "```bash\n",
    "git clone https://github.com/avalanche-strategy/D2D.git\n",
    "cd D2D\n",
    "```\n",
    "2. Create and activate the Conda environment:\n",
    "```bash\n",
    "conda env create -f environment.yml\n",
    "conda activate d2d\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## Environment Configuration\n",
    "To use the OpenAI and Anthropic API, you need to set up an environment variable for your API key. Create a `.env` file in the root directory of the project with the following content:\n",
    "\n",
    "```bash\n",
    "OPENAI_API_KEY=[Please replace this with your OPENAI API key]\n",
    "ANTHROPIC_API_KEY=[Please replace this with your ANTHROPIC API key]\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### Data Placement\n",
    "\n",
    "To ensure smooth operation of the processor, please organize your data as follows:\n",
    "\n",
    "- **Interview Data Structure (for processor)**:\n",
    "  Each interview should have its own subdirectory. The name of this subdirectory is the **interview_xxx** (e.g., `interview_1090` is a folder containing interview transcript files for 1090 theme.). While it is suggested to place these directories under `data/private_data/` for confidentiality, you may choose a different location if needed.\n",
    "\n",
    "- **Transcript Files (for processor)**:\n",
    "  Place all transcript files directly inside the interview directory. For example:\n",
    "  - `data/private_data/interview_1090/transcript1.txt`\n",
    "  - `data/private_data/interview_1090/transcript2.txt`\n",
    "  - etc.\n",
    "\n",
    "- **Guidelines CSV File (for processor)**:\n",
    "  A CSV file named `interview_xxx_guidelines.csv` containing the guideline questions. There should be a column named `guide_test` with the guideline questions. For example:\n",
    "  - `interview_1090_guidelines.csv` contains the guideline questions for the 1090 theme.\n",
    "\n",
    "- **Reference Answer (for evaluation)**: A CSV file named `response_xxx.csv` containing the reference answers for the guideline questions. The first column should be `respondent_id`, and the remaining columns should be the reference answers to the corresponding guideline questions. For example:\n",
    "  - `response_1090.csv` contains the reference answers for the 1090 theme.\n",
    "\n",
    "---\n",
    "\n",
    "## How It Works (Processor part)\n",
    "\n",
    "The processor follows these steps:\n",
    "1. **Segmentation**: Divides the transcript into question-response pairs.\n",
    "2. **Summarization**: Summarize the questions in the transcript and guideline questions.\n",
    "3. **Embedding**: Uses a SentenceTransformer model to embed summarized questions in the transcript and guideline questions.\n",
    "4. **Matching**: Matches segments to guideline questions via cosine similarity.\n",
    "5. **Summarization**: Summarizes matched segments using an LLM.\n",
    "6. **Output**: Generates a CSV with summaries, plus JSON metadata and a log file.\n",
    "\n",
    "---\n",
    "\n",
    "## Usage\n",
    "\n",
    "To run the processor on the synthetic data, use the following command after setting up your environment and data:\n",
    "\n",
    "```bash\n",
    "python examples/api_test/processor_test.py\n",
    "```\n",
    "---\n",
    "\n",
    "## Data Output\n",
    "\n",
    "Upon processing, the D2D pipeline generates four output files, which are stored in a user-designated directory. In the examples, this directory is `results/`, but users can specify any preferred path when running the processor. For the synthetic interview dataset `interview_food`, processed at 11:13 AM PDT on May 26, 2025, the following files are generated:\n",
    "\n",
    "- **`D2D_survey_food_generator_log_202505261113.txt`**\n",
    "  - **Description**: This plain text file contains the retrieval information passed into the generator module for the `interview_food` dataset, specifically for use in the evaluation process. It logs the data or queries that the generator utilizes to produce synthetic or processed outputs.\n",
    "  - **Purpose**: It supports the evaluation module by providing transparency into the inputs fed to the generator, aiding in assessing its performance or accuracy.\n",
    "\n",
    "- **`D2D_survey_food_log_202505261113.log`**\n",
    "  - **Description**: This log file records the processing steps applied to the `interview_food` dataset within the D2D pipeline. It captures details such as segmentation, embedding, matching, and summarization, along with any informational messages, warnings, or errors encountered during execution.\n",
    "  - **Purpose**: It acts as a comprehensive record of the workflow, facilitating debugging and ensuring that each processing step is documented for review or troubleshooting.\n",
    "\n",
    "- **`D2D_survey_food_references_202505261113.json`**\n",
    "  - **Description**: This JSON file provides reference information for the `interview_food` dataset, specifying the exact lines in the transcripts from which responses were extracted for each interview. It serves as a mapping to link summarized responses back to their original context within the source transcripts.\n",
    "  - **Purpose**: It enables users to trace and verify the origins of the extracted responses, offering insight into the context and accuracy of the summarization process.\n",
    "\n",
    "- **`D2D_survey_food_responses_202505261113.csv`**\n",
    "  - **Description**: This CSV file delivers the structured output of the D2D pipeline for the `interview_food` dataset. Each row typically corresponds to a single transcript, with columns containing summarized responses aligned with the guideline questions from the interview.\n",
    "  - **Purpose**: It provides a concise, tabular summary of key insights from the `interview_food` transcripts, making the data readily accessible for analysis or further use.\n",
    "\n",
    "**Note**: The timestamp in the file names (e.g., `202505261113`) is based on the date and time when the processing was run, ensuring unique file names for each execution and preventing overwrites.\n",
    "\n",
    "---\n",
    "\n",
    "## Explanation of the API\n",
    "**Note: for more detailed and executable examples, please refer to `examples/api_test/processor_test.py`. The first line of following example doesn't work until the package is pushed to PyPI**  \n",
    "\n",
    "### API Example\n",
    "Here’s how to process a sample transcript:\n",
    "\n",
    "```python\n",
    "# Step 0: Import the processor correctly.\n",
    "from d2d.processor import D2DProcessor\n",
    "\n",
    "# Suppose You have correctly imported the processor\n",
    "# Step 1: Initialize the processor\n",
    "processor = D2DProcessor(\n",
    "    llm_model=\"gpt-4o-mini\",\n",
    "    embedding_model=\"multi-qa-mpnet-base-dot-v1\",\n",
    "    max_concurrent_calls=10,\n",
    "    sampling_method=D2DProcessor.SamplingMethod.TOP_K,\n",
    ")\n",
    "\n",
    "# Step 2: Define paths relative to the root directory\n",
    "root_dir = os.path.abspath(os.path.join(os.path.dirname(__file__), '../..'))\n",
    "data_dir = os.path.join(root_dir, \"data\", \"synthetic_data\")\n",
    "interview_name = \"interview_food\"\n",
    "output_dir = os.path.join(root_dir, \"results\")\n",
    "\n",
    "# Step 3: Start transcripts processing\n",
    "processor.process_transcripts(\n",
    "    data_dir=data_dir,\n",
    "    interview_name=interview_name,\n",
    "    output_dir=output_dir,\n",
    "    disable_logging=False\n",
    "    )\n",
    "# Process completed\n",
    "```\n",
    "\n",
    "### Parameters\n",
    "\n",
    "#### Initialization Parameters `D2DProcessor`\n",
    "**Note that you don't have to set any parameters, each of them has a default setting. You can simply run `processor = D2DProcessor()`**\n",
    "\n",
    "- **llm_model : str**\n",
    "  The Large Language Model to use for summarization (e.g., \"gpt-4o-mini\"). This specifies the model that processes the text data.\n",
    "\n",
    "- **embedding_model : str**\n",
    "  The SentenceTransformer model used for embedding transcript segments and guideline questions (e.g., \"multi-qa-mpnet-base-dot-v1\"). This model converts text into vector representations for similarity matching.\n",
    "\n",
    "- **max_concurrent_calls : int**\n",
    "  Maximum number of concurrent API calls to the LLM, enabling efficient parallel processing of transcript segments.\n",
    "\n",
    "- **sampling_method : D2DProcessor.SamplingMethod**\n",
    "  Determines the method for sampling matched segments from the transcript:\n",
    "  - `TOP_K`: Selects the top K segments based on similarity scores.\n",
    "  - `TOP_P`: Selects segments until the cumulative probability reaches a specified threshold P.\n",
    "\n",
    "- **top_k : int, optional**\n",
    "  Number of top segments to consider when using `TOP_K` sampling. Default is 5. Ignored if `sampling_method` is `TOP_P`.\n",
    "\n",
    "- **top_p : float, optional**\n",
    "  Cumulative probability threshold for `TOP_P` sampling. Default is 0.5. Ignored if `sampling_method` is `TOP_K`.\n",
    "\n",
    "- **custom_extract_prompt : str, optional**\n",
    "  Custom prompt template for extracting key phrases from matched segments. The template can include `{context}` (the transcript segment) and `{query}` (the guideline question). If not provided, a default extraction prompt is used. Example:\n",
    "  `\"Using the dialogue: {context}, find a short phrase from the interviewee that answers '{query}'. Avoid pronouns and use explicit names. If no answer is found, return '[No answer found]'.\"`\n",
    "\n",
    "- **custom_summarize_prompt : str, optional**\n",
    "  Custom prompt template for summarizing extracted phrases. The template can include `{extracted_phrase}` (the extracted text) and `{query}` (the guideline question). If not provided, a default summarization prompt is used. Example:\n",
    "  `\"From the phrase: {extracted_phrase}, for the query '{query}', create a brief summary using only the original words, focusing on the main point.\"`\n",
    "\n",
    "#### Processing Parameters `process_transcripts`\n",
    "\n",
    "- **data_dir : str**\n",
    "  Directory containing the interview data, including transcript files and a guideline CSV file. This is the root path for input data.\n",
    "\n",
    "- **interview_name : str**\n",
    "  Name of the interview folder within `data_dir`. Specifies which interview’s data to process.\n",
    "\n",
    "- **output_dir : str**\n",
    "  Directory where output files (e.g., CSV, JSON, log files) will be saved. This is the destination for processed results.\n",
    "\n",
    "- **disable_logging : bool, optional**\n",
    "  If set to `True`, disables logging to both the console and log file. Default is `False`, enabling logging for debugging and tracking purposes.\n",
    "\n",
    "---\n",
    "\n",
    "## Additional Notes\n",
    "\n",
    "1. **Edge Cases**:\n",
    "   - Empty transcripts or missing guidelines will raise errors.\n",
    "   - Ensure API keys are set for LLM access.\n",
    "\n",
    "2. **Performance**:\n",
    "   - Processing time depends on transcript length and question count. Use GPU for faster computation.\n",
    "\n",
    "## Summary\n",
    "\n",
    "The `d2d` package simplifies the transformation of interview transcripts into structured data, offering:\n",
    "\n",
    "- **Automated Segmentation** and matching to guideline questions.\n",
    "- **LLM-Powered Summarization** for concise insights.\n",
    "- **Structured Outputs** for easy analysis.\n",
    "\n",
    "Explore D2D for your qualitative data needs!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0dc906a5-0d28-48b8-9e18-38a5bf1ca6af",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-26T03:04:34.349418Z",
     "start_time": "2025-05-26T03:04:34.346576Z"
    }
   },
   "source": [
    "**The following sections will replace the upper part when the package is published to PyPI**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5dfcc50-eb2e-44af-a702-57752d488b74",
   "metadata": {},
   "source": [
    "## Installation\n",
    "To start using the `d2d` package, install it via pip in your terminal:\n",
    "```bash\n",
    "pip install d2d\n",
    "```"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
