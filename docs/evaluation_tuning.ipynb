{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c6ed881b",
   "metadata": {},
   "source": [
    "## Adjusting the Similarity Threshold for Empty or Ambiguous Answers\n",
    "\n",
    "In the evaluation pipeline, if both the generated answer and the reference answer are vague, empty, or ambiguous, we should avoid penalizing the model with a low correctness or faithfulness score. Instead, we treat this as a **valid match**, indicating that the model appropriately responded with uncertainty when no answer was available.\n",
    "\n",
    "This behavior is implemented in the function `is_answer_empty_or_confused()`.\n",
    "\n",
    "### Function Location  \n",
    "This function is defined in `d2d/utils/eval_prompt_utils.py`, and used in `d2d/evaluation/ragas_eval.py` inside the `score_ragas()` function.\n",
    "\n",
    "### Working Mechanism  \n",
    "This function uses **embedding similarity** to compare a given answer against a list of pre-defined vague/confused templates such as:\n",
    "- \"I don't know\"\n",
    "- \"No idea\"\n",
    "- \"This is unclear\"\n",
    "- \"There is no information\"\n",
    "- \"N/A\"\n",
    "\n",
    "### Set the Similarity Threshold\n",
    "In `d2d/utils/eval_prompt_utils.py`, the similarity threshold for detecting vague or ambiguous answers is defined as:\n",
    "```python\n",
    "SIMILARITY_THRESHOLD = 0.78\n",
    "```\n",
    "This constant controls how strictly we classify an answer as \"empty\", \"confused\", or \"non-informative\".\n",
    "\n",
    "### Suggested Values\n",
    "\n",
    "We recommend tuning the similarity threshold based on your use case. Below is a guideline for selecting appropriate values:\n",
    "\n",
    "| Threshold      | Behavior                                      |\n",
    "|----------------|-----------------------------------------------|\n",
    "| `> 0.85`       | **Very strict**: fewer false positives, but might miss genuinely vague answers |\n",
    "| `0.75â€“0.80`    | **Balanced** (default = 0.78): good trade-off between sensitivity and specificity |\n",
    "| `< 0.70`       | **Very lenient**: more answers flagged as vague, may misclassify meaningful responses |"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04cb9221",
   "metadata": {},
   "source": [
    "## Modifying Evaluation Prompts\n",
    "\n",
    "Prompts sent to the LLM for scoring are constructed by `build_prompt(metric, row, chunk_list)`.\n",
    "\n",
    "### Function Location  \n",
    "This function is defined in `d2d/utils/eval_prompt_utils.py`, and called in `d2d/evaluation/ragas_eval.py` by `score_ragas()` function within the scoring loop per metric.\n",
    "\n",
    "### Adjust the Prompt\n",
    "You can modify the template inside `build_prompt()` to:\n",
    "- Change the wording\n",
    "- Provide more explicit scoring instructions\n",
    "- Add few-shot examples for more robust scoring behavior\n",
    "- Emphasize precision/recall/factual constraints\n",
    "\n",
    "Prompt example:\n",
    "```python\n",
    "prompt = f\"\"\"\n",
    "You are evaluating the {metric} of a generated answer.\n",
    "Question: {question}\n",
    "Reference Answer: {answer_ref}\n",
    "Generated Answer: {answer_rag}\n",
    "Context: {context}  # Optional\n",
    "Please assign a score between 1 and 5 and explain your reasoning.\n",
    "\"\"\"\n",
    "```\n",
    "\n",
    "Tip: Keep temperature = 0 for deterministic scoring."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce5f75bc",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
